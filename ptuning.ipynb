{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b95b45f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ys/anaconda3/envs/chatglm_test/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/ys/anaconda3/envs/chatglm_test/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/ys/anaconda3/envs/chatglm_test/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoModel,AutoTokenizer,Trainer,TrainingArguments,DataCollatorForSeq2Seq,AutoModelForCausalLM\n",
    "import datasets\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2be8e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = pd.read_json('/home/ys/zhy/chatglm微调/chat-dataset-baseline-main/history/其他中文问题补充/三国问题.json')\n",
    "ds = pd.read_json('./alpaca/alpaca_gpt4_data_zh.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00d78e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>保持健康的三个提示。</td>\n",
       "      <td></td>\n",
       "      <td>以下是保持健康的三个提示：\\n\\n1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>三原色是什么？</td>\n",
       "      <td></td>\n",
       "      <td>三原色通常指的是红色、绿色和蓝色（RGB）。它们是通过加色混合原理创建色彩的三种基础颜色。在...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>描述原子的结构。</td>\n",
       "      <td></td>\n",
       "      <td>原子是物质的基本单位，它由三种基本粒子组成：质子、中子和电子。质子和中子形成原子核，位于原子...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>如何减少空气污染？</td>\n",
       "      <td></td>\n",
       "      <td>有很多方法来减少空气污染:\\n\\n1. 减少机动车排放: 通过支持可再生能源来驱动的交通工具...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>描述一次你不得不做出困难决定的经历。</td>\n",
       "      <td></td>\n",
       "      <td>作为一名人工智能助手，我是由程序控制的，没有感情和个人意识，我的一切决策都是基于我的算法和程...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48813</th>\n",
       "      <td>生成一份烹饪专业人士的简历示例。</td>\n",
       "      <td></td>\n",
       "      <td>以下是一份烹饪专业人士的简历示例：\\n\\n个人信息\\n姓名: 王大厨 \\n联系电话: 123...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48814</th>\n",
       "      <td>蛋糕，我，吃</td>\n",
       "      <td></td>\n",
       "      <td>好的，你可以去吃蛋糕。享受美食！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48815</th>\n",
       "      <td>写一段关于著名人物米歇尔·奥巴马的介绍。</td>\n",
       "      <td></td>\n",
       "      <td>米歇尔·奥巴马是一位杰出的律师、公益活动家和作家，她曾在2009年到2017年间担任美国第一...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48816</th>\n",
       "      <td>考虑职业转换时应记住的五件事的清单。</td>\n",
       "      <td></td>\n",
       "      <td>当考虑进行职业转换时，以下是值得记住的五件事情：\\n\\n1. 研究新职业: 在转型到新的职业...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48817</th>\n",
       "      <td>分析给定的法律文件并解释关键点。</td>\n",
       "      <td></td>\n",
       "      <td>非常抱歉，你没有提供法律文件，所以我无法为你进行分析。 请提供具体的法律文件，我将尽力为你解...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48818 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                instruction input  \\\n",
       "0                保持健康的三个提示。         \n",
       "1                   三原色是什么？         \n",
       "2                  描述原子的结构。         \n",
       "3                 如何减少空气污染？         \n",
       "4        描述一次你不得不做出困难决定的经历。         \n",
       "...                     ...   ...   \n",
       "48813      生成一份烹饪专业人士的简历示例。         \n",
       "48814                蛋糕，我，吃         \n",
       "48815  写一段关于著名人物米歇尔·奥巴马的介绍。         \n",
       "48816    考虑职业转换时应记住的五件事的清单。         \n",
       "48817      分析给定的法律文件并解释关键点。         \n",
       "\n",
       "                                                  output  \n",
       "0      以下是保持健康的三个提示：\\n\\n1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游...  \n",
       "1      三原色通常指的是红色、绿色和蓝色（RGB）。它们是通过加色混合原理创建色彩的三种基础颜色。在...  \n",
       "2      原子是物质的基本单位，它由三种基本粒子组成：质子、中子和电子。质子和中子形成原子核，位于原子...  \n",
       "3      有很多方法来减少空气污染:\\n\\n1. 减少机动车排放: 通过支持可再生能源来驱动的交通工具...  \n",
       "4      作为一名人工智能助手，我是由程序控制的，没有感情和个人意识，我的一切决策都是基于我的算法和程...  \n",
       "...                                                  ...  \n",
       "48813  以下是一份烹饪专业人士的简历示例：\\n\\n个人信息\\n姓名: 王大厨 \\n联系电话: 123...  \n",
       "48814                                   好的，你可以去吃蛋糕。享受美食！  \n",
       "48815  米歇尔·奥巴马是一位杰出的律师、公益活动家和作家，她曾在2009年到2017年间担任美国第一...  \n",
       "48816  当考虑进行职业转换时，以下是值得记住的五件事情：\\n\\n1. 研究新职业: 在转型到新的职业...  \n",
       "48817  非常抱歉，你没有提供法律文件，所以我无法为你进行分析。 请提供具体的法律文件，我将尽力为你解...  \n",
       "\n",
       "[48818 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b875d7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_pandas(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0387855d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': ['保持健康的三个提示。', '三原色是什么？', '描述原子的结构。'],\n",
       " 'input': ['', '', ''],\n",
       " 'output': ['以下是保持健康的三个提示：\\n\\n1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游泳，能促进心血管健康，增强肌肉力量，并有助于减少体重。\\n\\n2. 均衡饮食。每天食用新鲜的蔬菜、水果、全谷物和脂肪含量低的蛋白质食物，避免高糖、高脂肪和加工食品，以保持健康的饮食习惯。\\n\\n3. 睡眠充足。睡眠对人体健康至关重要，成年人每天应保证 7-8 小时的睡眠。良好的睡眠有助于减轻压力，促进身体恢复，并提高注意力和记忆力。',\n",
       "  '三原色通常指的是红色、绿色和蓝色（RGB）。它们是通过加色混合原理创建色彩的三种基础颜色。在以发光为基础的显示设备中（如电视、计算机显示器、智能手机和平板电脑显示屏）, 三原色可混合产生大量色彩。其中红色和绿色可以混合生成黄色，红色和蓝色可以混合生成品红色，蓝色和绿色可以混合生成青色。当红色、绿色和蓝色按相等比例混合时，可以产生白色或灰色。\\n\\n此外，在印刷和绘画中，三原色指的是以颜料为基础的红、黄和蓝颜色（RYB）。这三种颜色用以通过减色混合原理来创建色彩。不过，三原色的具体定义并不唯一，不同的颜色系统可能会采用不同的三原色。',\n",
       "  '原子是物质的基本单位，它由三种基本粒子组成：质子、中子和电子。质子和中子形成原子核，位于原子中心，核外的电子围绕着原子核运动。\\n\\n原子结构具有层次性。原子核中，质子带正电，中子不带电（中性）。原子核非常小且致密，占据了原子总质量的绝大部分。电子带负电，通常围绕核运动，形成若干层次，称为壳层或电子层。电子数量与质子数量相等，使原子呈电中性。\\n\\n电子在每个壳层中都呈规律分布，并且不同壳层所能容纳的电子数也不同。在最里面的壳层一般只能容纳2个电子，其次一层最多可容纳8个电子，再往外的壳层可容纳的电子数逐层递增。\\n\\n原子核主要受到两种相互作用力的影响：强力和电磁力。强力的作用范围非常小，主要限制在原子核内，具有极强的吸引作用，使核子（质子和中子）紧密结合在一起。电磁力的作用范围较大，主要通过核外的电子与原子核相互作用，发挥作用。\\n\\n这就是原子的基本结构。原子内部结构复杂多样，不同元素的原子核中质子、中子数量不同，核外电子排布分布也不同，形成了丰富多彩的化学世界。']}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "634805bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_Path = './bloom1b4/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e40de618",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_Path,trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_Path,low_cpu_mem_usage = True,load_in_8bit = False,trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "829c5705",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ys/anaconda3/envs/chatglm_test/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/ys/anaconda3/envs/chatglm_test/lib/python3.10/site-packages/transformers/generation/utils.py:1636: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m政治这个词是什么意思?请详细解释\u001b[39m\u001b[38;5;124m'\u001b[39m,truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(input_ids \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m],)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "inputs = tokenizer('政治这个词是什么意思?请详细解释',truncation=True,max_length=512,return_tensors='pt')\n",
    "output = model.generate(input_ids = inputs['input_ids'],)\n",
    "assert 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bcd557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.decode(output[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d936893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'政治这个词是什么意思?请详细解释一下。</s>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17c510f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.word_embeddings.weight torch.Size([46145, 2048]) torch.float16\n",
      "transformer.word_embeddings_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.word_embeddings_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.0.input_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.0.input_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.0.self_attention.query_key_value.weight torch.Size([6144, 2048]) torch.int8\n",
      "transformer.h.0.self_attention.query_key_value.bias torch.Size([6144]) torch.float16\n",
      "transformer.h.0.self_attention.dense.weight torch.Size([2048, 2048]) torch.int8\n",
      "transformer.h.0.self_attention.dense.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.0.post_attention_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.0.post_attention_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.0.mlp.dense_h_to_4h.weight torch.Size([8192, 2048]) torch.int8\n",
      "transformer.h.0.mlp.dense_h_to_4h.bias torch.Size([8192]) torch.float16\n",
      "transformer.h.0.mlp.dense_4h_to_h.weight torch.Size([2048, 8192]) torch.int8\n",
      "transformer.h.0.mlp.dense_4h_to_h.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.1.input_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.1.input_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.1.self_attention.query_key_value.weight torch.Size([6144, 2048]) torch.int8\n",
      "transformer.h.1.self_attention.query_key_value.bias torch.Size([6144]) torch.float16\n",
      "transformer.h.1.self_attention.dense.weight torch.Size([2048, 2048]) torch.int8\n",
      "transformer.h.1.self_attention.dense.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.1.post_attention_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.1.post_attention_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.1.mlp.dense_h_to_4h.weight torch.Size([8192, 2048]) torch.int8\n",
      "transformer.h.1.mlp.dense_h_to_4h.bias torch.Size([8192]) torch.float16\n",
      "transformer.h.1.mlp.dense_4h_to_h.weight torch.Size([2048, 8192]) torch.int8\n",
      "transformer.h.1.mlp.dense_4h_to_h.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.2.input_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.2.input_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.2.self_attention.query_key_value.weight torch.Size([6144, 2048]) torch.int8\n",
      "transformer.h.2.self_attention.query_key_value.bias torch.Size([6144]) torch.float16\n",
      "transformer.h.2.self_attention.dense.weight torch.Size([2048, 2048]) torch.int8\n",
      "transformer.h.2.self_attention.dense.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.2.post_attention_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.2.post_attention_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.2.mlp.dense_h_to_4h.weight torch.Size([8192, 2048]) torch.int8\n",
      "transformer.h.2.mlp.dense_h_to_4h.bias torch.Size([8192]) torch.float16\n",
      "transformer.h.2.mlp.dense_4h_to_h.weight torch.Size([2048, 8192]) torch.int8\n",
      "transformer.h.2.mlp.dense_4h_to_h.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.3.input_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.3.input_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.3.self_attention.query_key_value.weight torch.Size([6144, 2048]) torch.int8\n",
      "transformer.h.3.self_attention.query_key_value.bias torch.Size([6144]) torch.float16\n",
      "transformer.h.3.self_attention.dense.weight torch.Size([2048, 2048]) torch.int8\n",
      "transformer.h.3.self_attention.dense.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.3.post_attention_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.3.post_attention_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.3.mlp.dense_h_to_4h.weight torch.Size([8192, 2048]) torch.int8\n",
      "transformer.h.3.mlp.dense_h_to_4h.bias torch.Size([8192]) torch.float16\n",
      "transformer.h.3.mlp.dense_4h_to_h.weight torch.Size([2048, 8192]) torch.int8\n",
      "transformer.h.3.mlp.dense_4h_to_h.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.4.input_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.4.input_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.4.self_attention.query_key_value.weight torch.Size([6144, 2048]) torch.int8\n",
      "transformer.h.4.self_attention.query_key_value.bias torch.Size([6144]) torch.float16\n",
      "transformer.h.4.self_attention.dense.weight torch.Size([2048, 2048]) torch.int8\n",
      "transformer.h.4.self_attention.dense.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.4.post_attention_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.4.post_attention_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.4.mlp.dense_h_to_4h.weight torch.Size([8192, 2048]) torch.int8\n",
      "transformer.h.4.mlp.dense_h_to_4h.bias torch.Size([8192]) torch.float16\n",
      "transformer.h.4.mlp.dense_4h_to_h.weight torch.Size([2048, 8192]) torch.int8\n",
      "transformer.h.4.mlp.dense_4h_to_h.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.5.input_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.5.input_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.5.self_attention.query_key_value.weight torch.Size([6144, 2048]) torch.int8\n",
      "transformer.h.5.self_attention.query_key_value.bias torch.Size([6144]) torch.float16\n",
      "transformer.h.5.self_attention.dense.weight torch.Size([2048, 2048]) torch.int8\n",
      "transformer.h.5.self_attention.dense.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.5.post_attention_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.5.post_attention_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.5.mlp.dense_h_to_4h.weight torch.Size([8192, 2048]) torch.int8\n",
      "transformer.h.5.mlp.dense_h_to_4h.bias torch.Size([8192]) torch.float16\n",
      "transformer.h.5.mlp.dense_4h_to_h.weight torch.Size([2048, 8192]) torch.int8\n",
      "transformer.h.5.mlp.dense_4h_to_h.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.6.input_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.6.input_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.6.self_attention.query_key_value.weight torch.Size([6144, 2048]) torch.int8\n",
      "transformer.h.6.self_attention.query_key_value.bias torch.Size([6144]) torch.float16\n",
      "transformer.h.6.self_attention.dense.weight torch.Size([2048, 2048]) torch.int8\n",
      "transformer.h.6.self_attention.dense.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.6.post_attention_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.6.post_attention_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.6.mlp.dense_h_to_4h.weight torch.Size([8192, 2048]) torch.int8\n",
      "transformer.h.6.mlp.dense_h_to_4h.bias torch.Size([8192]) torch.float16\n",
      "transformer.h.6.mlp.dense_4h_to_h.weight torch.Size([2048, 8192]) torch.int8\n",
      "transformer.h.6.mlp.dense_4h_to_h.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.7.input_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.7.input_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.7.self_attention.query_key_value.weight torch.Size([6144, 2048]) torch.int8\n",
      "transformer.h.7.self_attention.query_key_value.bias torch.Size([6144]) torch.float16\n",
      "transformer.h.7.self_attention.dense.weight torch.Size([2048, 2048]) torch.int8\n",
      "transformer.h.7.self_attention.dense.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.7.post_attention_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.7.post_attention_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.7.mlp.dense_h_to_4h.weight torch.Size([8192, 2048]) torch.int8\n",
      "transformer.h.7.mlp.dense_h_to_4h.bias torch.Size([8192]) torch.float16\n",
      "transformer.h.7.mlp.dense_4h_to_h.weight torch.Size([2048, 8192]) torch.int8\n",
      "transformer.h.7.mlp.dense_4h_to_h.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.8.input_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.8.input_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.8.self_attention.query_key_value.weight torch.Size([6144, 2048]) torch.int8\n",
      "transformer.h.8.self_attention.query_key_value.bias torch.Size([6144]) torch.float16\n",
      "transformer.h.8.self_attention.dense.weight torch.Size([2048, 2048]) torch.int8\n",
      "transformer.h.8.self_attention.dense.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.8.post_attention_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.8.post_attention_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.8.mlp.dense_h_to_4h.weight torch.Size([8192, 2048]) torch.int8\n",
      "transformer.h.8.mlp.dense_h_to_4h.bias torch.Size([8192]) torch.float16\n",
      "transformer.h.8.mlp.dense_4h_to_h.weight torch.Size([2048, 8192]) torch.int8\n",
      "transformer.h.8.mlp.dense_4h_to_h.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.9.input_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.9.input_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.9.self_attention.query_key_value.weight torch.Size([6144, 2048]) torch.int8\n",
      "transformer.h.9.self_attention.query_key_value.bias torch.Size([6144]) torch.float16\n",
      "transformer.h.9.self_attention.dense.weight torch.Size([2048, 2048]) torch.int8\n",
      "transformer.h.9.self_attention.dense.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.9.post_attention_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.9.post_attention_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.9.mlp.dense_h_to_4h.weight torch.Size([8192, 2048]) torch.int8\n",
      "transformer.h.9.mlp.dense_h_to_4h.bias torch.Size([8192]) torch.float16\n",
      "transformer.h.9.mlp.dense_4h_to_h.weight torch.Size([2048, 8192]) torch.int8\n",
      "transformer.h.9.mlp.dense_4h_to_h.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.10.input_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.10.input_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.10.self_attention.query_key_value.weight torch.Size([6144, 2048]) torch.int8\n",
      "transformer.h.10.self_attention.query_key_value.bias torch.Size([6144]) torch.float16\n",
      "transformer.h.10.self_attention.dense.weight torch.Size([2048, 2048]) torch.int8\n",
      "transformer.h.10.self_attention.dense.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.10.post_attention_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.10.post_attention_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.10.mlp.dense_h_to_4h.weight torch.Size([8192, 2048]) torch.int8\n",
      "transformer.h.10.mlp.dense_h_to_4h.bias torch.Size([8192]) torch.float16\n",
      "transformer.h.10.mlp.dense_4h_to_h.weight torch.Size([2048, 8192]) torch.int8\n",
      "transformer.h.10.mlp.dense_4h_to_h.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.11.input_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.11.input_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.11.self_attention.query_key_value.weight torch.Size([6144, 2048]) torch.int8\n",
      "transformer.h.11.self_attention.query_key_value.bias torch.Size([6144]) torch.float16\n",
      "transformer.h.11.self_attention.dense.weight torch.Size([2048, 2048]) torch.int8\n",
      "transformer.h.11.self_attention.dense.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.11.post_attention_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.11.post_attention_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.11.mlp.dense_h_to_4h.weight torch.Size([8192, 2048]) torch.int8\n",
      "transformer.h.11.mlp.dense_h_to_4h.bias torch.Size([8192]) torch.float16\n",
      "transformer.h.11.mlp.dense_4h_to_h.weight torch.Size([2048, 8192]) torch.int8\n",
      "transformer.h.11.mlp.dense_4h_to_h.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.12.input_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.12.input_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.12.self_attention.query_key_value.weight torch.Size([6144, 2048]) torch.int8\n",
      "transformer.h.12.self_attention.query_key_value.bias torch.Size([6144]) torch.float16\n",
      "transformer.h.12.self_attention.dense.weight torch.Size([2048, 2048]) torch.int8\n",
      "transformer.h.12.self_attention.dense.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.12.post_attention_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.12.post_attention_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.12.mlp.dense_h_to_4h.weight torch.Size([8192, 2048]) torch.int8\n",
      "transformer.h.12.mlp.dense_h_to_4h.bias torch.Size([8192]) torch.float16\n",
      "transformer.h.12.mlp.dense_4h_to_h.weight torch.Size([2048, 8192]) torch.int8\n",
      "transformer.h.12.mlp.dense_4h_to_h.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.13.input_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.13.input_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.13.self_attention.query_key_value.weight torch.Size([6144, 2048]) torch.int8\n",
      "transformer.h.13.self_attention.query_key_value.bias torch.Size([6144]) torch.float16\n",
      "transformer.h.13.self_attention.dense.weight torch.Size([2048, 2048]) torch.int8\n",
      "transformer.h.13.self_attention.dense.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.13.post_attention_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.13.post_attention_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.13.mlp.dense_h_to_4h.weight torch.Size([8192, 2048]) torch.int8\n",
      "transformer.h.13.mlp.dense_h_to_4h.bias torch.Size([8192]) torch.float16\n",
      "transformer.h.13.mlp.dense_4h_to_h.weight torch.Size([2048, 8192]) torch.int8\n",
      "transformer.h.13.mlp.dense_4h_to_h.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.14.input_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.14.input_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.14.self_attention.query_key_value.weight torch.Size([6144, 2048]) torch.int8\n",
      "transformer.h.14.self_attention.query_key_value.bias torch.Size([6144]) torch.float16\n",
      "transformer.h.14.self_attention.dense.weight torch.Size([2048, 2048]) torch.int8\n",
      "transformer.h.14.self_attention.dense.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.14.post_attention_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.14.post_attention_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.14.mlp.dense_h_to_4h.weight torch.Size([8192, 2048]) torch.int8\n",
      "transformer.h.14.mlp.dense_h_to_4h.bias torch.Size([8192]) torch.float16\n",
      "transformer.h.14.mlp.dense_4h_to_h.weight torch.Size([2048, 8192]) torch.int8\n",
      "transformer.h.14.mlp.dense_4h_to_h.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.15.input_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.15.input_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.15.self_attention.query_key_value.weight torch.Size([6144, 2048]) torch.int8\n",
      "transformer.h.15.self_attention.query_key_value.bias torch.Size([6144]) torch.float16\n",
      "transformer.h.15.self_attention.dense.weight torch.Size([2048, 2048]) torch.int8\n",
      "transformer.h.15.self_attention.dense.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.15.post_attention_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.15.post_attention_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.15.mlp.dense_h_to_4h.weight torch.Size([8192, 2048]) torch.int8\n",
      "transformer.h.15.mlp.dense_h_to_4h.bias torch.Size([8192]) torch.float16\n",
      "transformer.h.15.mlp.dense_4h_to_h.weight torch.Size([2048, 8192]) torch.int8\n",
      "transformer.h.15.mlp.dense_4h_to_h.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.16.input_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.16.input_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.16.self_attention.query_key_value.weight torch.Size([6144, 2048]) torch.int8\n",
      "transformer.h.16.self_attention.query_key_value.bias torch.Size([6144]) torch.float16\n",
      "transformer.h.16.self_attention.dense.weight torch.Size([2048, 2048]) torch.int8\n",
      "transformer.h.16.self_attention.dense.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.16.post_attention_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.16.post_attention_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.16.mlp.dense_h_to_4h.weight torch.Size([8192, 2048]) torch.int8\n",
      "transformer.h.16.mlp.dense_h_to_4h.bias torch.Size([8192]) torch.float16\n",
      "transformer.h.16.mlp.dense_4h_to_h.weight torch.Size([2048, 8192]) torch.int8\n",
      "transformer.h.16.mlp.dense_4h_to_h.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.17.input_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.17.input_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.17.self_attention.query_key_value.weight torch.Size([6144, 2048]) torch.int8\n",
      "transformer.h.17.self_attention.query_key_value.bias torch.Size([6144]) torch.float16\n",
      "transformer.h.17.self_attention.dense.weight torch.Size([2048, 2048]) torch.int8\n",
      "transformer.h.17.self_attention.dense.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.17.post_attention_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.17.post_attention_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.17.mlp.dense_h_to_4h.weight torch.Size([8192, 2048]) torch.int8\n",
      "transformer.h.17.mlp.dense_h_to_4h.bias torch.Size([8192]) torch.float16\n",
      "transformer.h.17.mlp.dense_4h_to_h.weight torch.Size([2048, 8192]) torch.int8\n",
      "transformer.h.17.mlp.dense_4h_to_h.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.18.input_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.18.input_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.18.self_attention.query_key_value.weight torch.Size([6144, 2048]) torch.int8\n",
      "transformer.h.18.self_attention.query_key_value.bias torch.Size([6144]) torch.float16\n",
      "transformer.h.18.self_attention.dense.weight torch.Size([2048, 2048]) torch.int8\n",
      "transformer.h.18.self_attention.dense.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.18.post_attention_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.18.post_attention_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.18.mlp.dense_h_to_4h.weight torch.Size([8192, 2048]) torch.int8\n",
      "transformer.h.18.mlp.dense_h_to_4h.bias torch.Size([8192]) torch.float16\n",
      "transformer.h.18.mlp.dense_4h_to_h.weight torch.Size([2048, 8192]) torch.int8\n",
      "transformer.h.18.mlp.dense_4h_to_h.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.19.input_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.19.input_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.19.self_attention.query_key_value.weight torch.Size([6144, 2048]) torch.int8\n",
      "transformer.h.19.self_attention.query_key_value.bias torch.Size([6144]) torch.float16\n",
      "transformer.h.19.self_attention.dense.weight torch.Size([2048, 2048]) torch.int8\n",
      "transformer.h.19.self_attention.dense.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.19.post_attention_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.19.post_attention_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.19.mlp.dense_h_to_4h.weight torch.Size([8192, 2048]) torch.int8\n",
      "transformer.h.19.mlp.dense_h_to_4h.bias torch.Size([8192]) torch.float16\n",
      "transformer.h.19.mlp.dense_4h_to_h.weight torch.Size([2048, 8192]) torch.int8\n",
      "transformer.h.19.mlp.dense_4h_to_h.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.20.input_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.20.input_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.20.self_attention.query_key_value.weight torch.Size([6144, 2048]) torch.int8\n",
      "transformer.h.20.self_attention.query_key_value.bias torch.Size([6144]) torch.float16\n",
      "transformer.h.20.self_attention.dense.weight torch.Size([2048, 2048]) torch.int8\n",
      "transformer.h.20.self_attention.dense.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.20.post_attention_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.20.post_attention_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.20.mlp.dense_h_to_4h.weight torch.Size([8192, 2048]) torch.int8\n",
      "transformer.h.20.mlp.dense_h_to_4h.bias torch.Size([8192]) torch.float16\n",
      "transformer.h.20.mlp.dense_4h_to_h.weight torch.Size([2048, 8192]) torch.int8\n",
      "transformer.h.20.mlp.dense_4h_to_h.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.21.input_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.21.input_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.21.self_attention.query_key_value.weight torch.Size([6144, 2048]) torch.int8\n",
      "transformer.h.21.self_attention.query_key_value.bias torch.Size([6144]) torch.float16\n",
      "transformer.h.21.self_attention.dense.weight torch.Size([2048, 2048]) torch.int8\n",
      "transformer.h.21.self_attention.dense.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.21.post_attention_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.21.post_attention_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.21.mlp.dense_h_to_4h.weight torch.Size([8192, 2048]) torch.int8\n",
      "transformer.h.21.mlp.dense_h_to_4h.bias torch.Size([8192]) torch.float16\n",
      "transformer.h.21.mlp.dense_4h_to_h.weight torch.Size([2048, 8192]) torch.int8\n",
      "transformer.h.21.mlp.dense_4h_to_h.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.22.input_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.22.input_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.22.self_attention.query_key_value.weight torch.Size([6144, 2048]) torch.int8\n",
      "transformer.h.22.self_attention.query_key_value.bias torch.Size([6144]) torch.float16\n",
      "transformer.h.22.self_attention.dense.weight torch.Size([2048, 2048]) torch.int8\n",
      "transformer.h.22.self_attention.dense.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.22.post_attention_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.22.post_attention_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.22.mlp.dense_h_to_4h.weight torch.Size([8192, 2048]) torch.int8\n",
      "transformer.h.22.mlp.dense_h_to_4h.bias torch.Size([8192]) torch.float16\n",
      "transformer.h.22.mlp.dense_4h_to_h.weight torch.Size([2048, 8192]) torch.int8\n",
      "transformer.h.22.mlp.dense_4h_to_h.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.23.input_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.23.input_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.23.self_attention.query_key_value.weight torch.Size([6144, 2048]) torch.int8\n",
      "transformer.h.23.self_attention.query_key_value.bias torch.Size([6144]) torch.float16\n",
      "transformer.h.23.self_attention.dense.weight torch.Size([2048, 2048]) torch.int8\n",
      "transformer.h.23.self_attention.dense.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.23.post_attention_layernorm.weight torch.Size([2048]) torch.float16\n",
      "transformer.h.23.post_attention_layernorm.bias torch.Size([2048]) torch.float16\n",
      "transformer.h.23.mlp.dense_h_to_4h.weight torch.Size([8192, 2048]) torch.int8\n",
      "transformer.h.23.mlp.dense_h_to_4h.bias torch.Size([8192]) torch.float16\n",
      "transformer.h.23.mlp.dense_4h_to_h.weight torch.Size([2048, 8192]) torch.int8\n",
      "transformer.h.23.mlp.dense_4h_to_h.bias torch.Size([2048]) torch.float16\n",
      "transformer.ln_f.weight torch.Size([2048]) torch.float16\n",
      "transformer.ln_f.bias torch.Size([2048]) torch.float16\n"
     ]
    }
   ],
   "source": [
    "for name,param in model.named_parameters():\n",
    "    print(name,param.shape,param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3712b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集处理\n",
    "max_seq_len = 512\n",
    "def preprocess(example):\n",
    "    context_id = tokenizer.encode(example['instruction'],truncation = True,max_length=max_seq_len)\n",
    "    target_id = tokenizer.encode(example['output'],truncation = True,max_length=max_seq_len,add_special_tokens = False)\n",
    "    input_ids = context_id + target_id + [tokenizer.eos_token_id]\n",
    "    return {'input_ids':input_ids,'context_len':len(context_id)}\n",
    "def data_collator(features:list):\n",
    "    len_id = [len(x['input_ids']) for x in features]\n",
    "    longest = max(len_id)\n",
    "    input_ids = []\n",
    "    label_ls = []\n",
    "    for length,feature in sorted(zip(len_id,features),key = lambda x:-x[0]):  # 按照batch中最长的input_ids进行padding\n",
    "        ids = feature['input_ids']\n",
    "        context_len = feature['context_len']\n",
    "\n",
    "        label = (\n",
    "            [-100]*(context_len - 1) + ids[(context_len - 1):] + [-100]*(longest-length) #  (instruct - > -100)   +target + padding\n",
    "        )\n",
    "        ids = ids + [tokenizer.pad_token_id] *(longest-length)\n",
    "        label_ls.append(torch.LongTensor(label))\n",
    "        input_ids.append(torch.LongTensor(ids))\n",
    "    input_ids = torch.stack(input_ids)\n",
    "    labels = torch.stack(label_ls)\n",
    "    return {'input_ids':input_ids,'labels':labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eaf8d7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 没有attention_mask的\n",
    "# max_seq_len = 512\n",
    "# def precess(example):\n",
    "#     context_id = tokenizer.encode(example['instruction'],max_length = max_seq_len,truncation = True)\n",
    "#     target_id = tokenizer.encode(example['output'],max_length = max_seq_len,add_special_tokens = False,truncation = True)\n",
    "#     input_ids = context_id + target_id + [tokenizer.eos_token_id]\n",
    "#     return {'input_ids':input_ids,'context_len':len(context_id),'target_len':len(target_id)}\n",
    "# def data_collator(features:list):\n",
    "#     len_id = [len(x['input_ids']) for x in features]\n",
    "#     longest = max(len_id)\n",
    "#     input_ids = []\n",
    "#     label_ls = []\n",
    "#     for length,feature in sorted(zip(len_id,features),key = lambda x:-x[0]):  # 按照batch中最长的input_ids进行padding\n",
    "#         ids = feature['input_ids']\n",
    "#         context_len = feature['context_len']\n",
    "\n",
    "#         label = (\n",
    "#             [-100]*(context_len - 1) + ids[(context_len - 1):] + [-100]*(longest-length) #  (instruct - > -100)   +target + padding\n",
    "#         )\n",
    "#         ids = ids + [tokenizer.pad_token_id] *(longest-length)\n",
    "#         label_ls.append(torch.tensor(label))\n",
    "#         input_ids.append(torch.tensor(ids))\n",
    "#     input_ids = torch.stack(input_ids)\n",
    "#     # print(input_ids)\n",
    "#     labels = torch.stack(label_ls)\n",
    "#     return {'input_ids':input_ids,'labels':labels}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed9f6a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 3783, 39155, 38495,    34,  2680, 11518,  9047]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "544c52a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(example):\n",
    "    MAX_LENGTH = 256\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = tokenizer(\"\\n\".join([\"Human: \" + example[\"instruction\"], example[\"input\"]]).strip() + \"\\n\\nAssistant: \")\n",
    "    response = tokenizer(example[\"output\"] + tokenizer.eos_token)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"]\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"]\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "25d4808a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd1b396743bd4e9e92435b9982951fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/48818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 48818\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_ds  = ds.map(process_func,remove_columns=ds.column_names)\n",
    "tokenizer_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4e377f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: 三原色是什么？\\n\\nAssistant: 三原色通常指的是红色、绿色和蓝色（RGB）。它们是通过加色混合原理创建色彩的三种基础颜色。在以发光为基础的显示设备中（如电视、计算机显示器、智能手机和平板电脑显示屏）, 三原色可混合产生大量色彩。其中红色和绿色可以混合生成黄色，红色和蓝色可以混合生成品红色，蓝色和绿色可以混合生成青色。当红色、绿色和蓝色按相等比例混合时，可以产生白色或灰色。\\n\\n此外，在印刷和绘画中，三原色指的是以颜料为基础的红、黄和蓝颜色（RYB）。这三种颜色用以通过减色混合原理来创建色彩。不过，三原色的具体定义并不唯一，不同的颜色系统可能会采用不同的三原色。</s>'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer_ds[1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e398d6a7",
   "metadata": {},
   "source": [
    "### Ptuning step 1 配置文件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f400c6b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptEncoderConfig(peft_type=<PeftType.P_TUNING: 'P_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, num_virtual_tokens=10, token_dim=None, num_transformer_submodules=None, num_attention_heads=None, num_layers=None, encoder_reparameterization_type=<PromptEncoderReparameterizationType.MLP: 'MLP'>, encoder_hidden_size=1024, encoder_num_layers=5, encoder_dropout=0.1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PromptEncoderConfig, TaskType, get_peft_model, PromptEncoderReparameterizationType\n",
    "\n",
    "config = PromptEncoderConfig(task_type=TaskType.CAUSAL_LM, num_virtual_tokens=10,\n",
    "                             encoder_reparameterization_type=PromptEncoderReparameterizationType.MLP,\n",
    "                             encoder_dropout=0.1, encoder_num_layers=5, encoder_hidden_size=1024)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "05b7ae51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ys/anaconda3/envs/chatglm_test/lib/python3.10/site-packages/peft/tuners/p_tuning/model.py:106: UserWarning: for MLP, the argument `encoder_num_layers` is ignored. Exactly 2 MLP layers are used.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5a399250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.supports_gradient_checkpointing = True\n",
    "# model.gradient_checkpointing_enable()\n",
    "# model.enable_input_require_grads()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "99903721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): BloomForCausalLM(\n",
       "    (transformer): BloomModel(\n",
       "      (word_embeddings): Embedding(46145, 2048)\n",
       "      (word_embeddings_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (h): ModuleList(\n",
       "        (0-23): 24 x BloomBlock(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attention): BloomAttention(\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): BloomMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (gelu_impl): BloomGelu()\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2048, out_features=46145, bias=False)\n",
       "  )\n",
       "  (prompt_encoder): ModuleDict(\n",
       "    (default): PromptEncoder(\n",
       "      (embedding): Embedding(10, 2048)\n",
       "      (mlp_head): Sequential(\n",
       "        (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (word_embeddings): Embedding(46145, 2048)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e809cf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,267,456 || all params: 1,308,379,136 || trainable%: 0.4025940077356905\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fbe11f63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ys/anaconda3/envs/chatglm_test/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23' max='6102' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  23/6102 00:06 < 33:43, 3.00 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.281700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.341800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.876200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.939000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.673500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.677000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.767300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.296000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.232900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.735500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.279700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.859400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.533900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.456600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.372900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.478800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.412400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.621300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.519000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 25\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 创建训练器\u001b[39;00m\n\u001b[1;32m     16\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     17\u001b[0m     model \u001b[38;5;241m=\u001b[39m model,\n\u001b[1;32m     18\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m tokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \n\u001b[1;32m     24\u001b[0m )\n\u001b[0;32m---> 25\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm_test/lib/python3.10/site-packages/transformers/trainer.py:1537\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm_test/lib/python3.10/site-packages/transformers/trainer.py:1854\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1853\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1854\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1857\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1859\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1860\u001b[0m ):\n\u001b[1;32m   1861\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm_test/lib/python3.10/site-packages/transformers/trainer.py:2735\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2735\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2738\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm_test/lib/python3.10/site-packages/transformers/trainer.py:2758\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2756\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2757\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2758\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2759\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2760\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm_test/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm_test/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm_test/lib/python3.10/site-packages/accelerate/utils/operations.py:825\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm_test/lib/python3.10/site-packages/accelerate/utils/operations.py:813\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm_test/lib/python3.10/site-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm_test/lib/python3.10/site-packages/peft/peft_model.py:1121\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1119\u001b[0m prompts \u001b[38;5;241m=\u001b[39m prompts\u001b[38;5;241m.\u001b[39mto(inputs_embeds\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m   1120\u001b[0m inputs_embeds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((prompts, inputs_embeds), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 1121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm_test/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm_test/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm_test/lib/python3.10/site-packages/transformers/models/bloom/modeling_bloom.py:858\u001b[0m, in \u001b[0;36mBloomForCausalLM.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **deprecated_arguments)\u001b[0m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unexpected arguments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeprecated_arguments\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    856\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 858\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    869\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    871\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm_test/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm_test/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm_test/lib/python3.10/site-packages/transformers/models/bloom/modeling_bloom.py:722\u001b[0m, in \u001b[0;36mBloomModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, **deprecated_arguments)\u001b[0m\n\u001b[1;32m    711\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    712\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    713\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    719\u001b[0m         output_attentions,\n\u001b[1;32m    720\u001b[0m     )\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 722\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m        \u001b[49m\u001b[43malibi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malibi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    732\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm_test/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm_test/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm_test/lib/python3.10/site-packages/transformers/models/bloom/modeling_bloom.py:410\u001b[0m, in \u001b[0;36mBloomBlock.forward\u001b[0;34m(self, hidden_states, alibi, attention_mask, layer_past, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    407\u001b[0m     residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    409\u001b[0m \u001b[38;5;66;03m# Self attention.\u001b[39;00m\n\u001b[0;32m--> 410\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayernorm_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43malibi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malibi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    423\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm_test/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm_test/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm_test/lib/python3.10/site-packages/transformers/models/bloom/modeling_bloom.py:268\u001b[0m, in \u001b[0;36mBloomAttention.forward\u001b[0;34m(self, hidden_states, residual, alibi, attention_mask, layer_past, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    264\u001b[0m (query_layer, key_layer, value_layer) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_heads(fused_qkv)\n\u001b[1;32m    266\u001b[0m batch_size, q_length, _, _ \u001b[38;5;241m=\u001b[39m query_layer\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 268\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[43mquery_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m key_layer \u001b[38;5;241m=\u001b[39m key_layer\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim, q_length)\n\u001b[1;32m    270\u001b[0m value_layer \u001b[38;5;241m=\u001b[39m value_layer\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, q_length, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 创建训练文件\n",
    "args = TrainingArguments(\n",
    "    output_dir = './',\n",
    "    per_device_train_batch_size = 2,\n",
    "    num_train_epochs =1,\n",
    "    weight_decay = 0.1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    lr_scheduler_type = 'linear',\n",
    "    learning_rate = 1e-5,\n",
    "    fp16 = True,\n",
    "    remove_unused_columns=True,\n",
    "    logging_steps=1,\n",
    "    \n",
    ")\n",
    "# 创建训练器\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = tokenizer_ds,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer,padding=True),\n",
    "    # data_collator=data_collator,\n",
    "    args = args,\n",
    "    \n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b411f209",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer('赵云是怎么死的？',return_tensors='pt').to(model.device)\n",
    "#inputs[0].numpy().tolist()\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02438a80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = model.generate(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659517b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31feba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e2697a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(output[0].cpu().numpy().tolist(),skip_special_tokens = True,max_length = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d0d5d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b26a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.merge_and_unload() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2069738",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(inputs.to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b10e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.merge_and_upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7383d98b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a8db5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b8fc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl = torch.utils.data.DataLoader(tokenizer_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a0592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.tensor(batch['input_ids']).to('cuda').shape,torch.tensor(batch['attention_mask']).to('cuda').shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73171f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for batch in dl:\n",
    "    x = model(torch.tensor(batch['input_ids']).to('cuda'),attention_mask = torch.tensor(batch['attention_mask']).to('cuda'))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0a5940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7a5879",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3f11d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a75c59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(['我爱不'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78748b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c167cff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6993bd53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32baad48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
